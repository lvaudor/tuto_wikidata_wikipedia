---
title: "Wikipedia_contenu_Lise"
format: html
editor: visual
---
```{r}
library(tidyverse)
```



On reprend le jeu de données des géographes français, et on se concentre sur celui qui fait l'objet du nombre le plus important d'articles.

```{r}
geographes_url=readRDS("data/geographes_url.RDS") %>% 
  filter(str_detect(link,"wikipedia"))
# On ne garde que les articles de Wikipédia et non par ex. Wikisource ou Wikiquote
geographes_url %>% 
  group_by(person,person_label) %>% 
  tally() %>% 
  arrange(desc(n))
```

Intéressons-nous à une personne en particulier: Elisée Reclus. 

```{r}
focus_ER=geographes_url %>% 
  filter(person_label=="Élisée Reclus")
```

# Récupération du texte d'un article

Ici, scraping "classique" avec le package rvest

```{r}
library(rvest)
link="https://fr.wikipedia.org/wiki/%C3%89lis%C3%A9e_Reclus"

html=read_html(link)

nodes=html_nodes(html,"h1,h2")
nodes
html_text(nodes)
```

On récupère tout le texte:

```{r}
nodes=html %>% 
  html_nodes("h1,h2,h3,h4,h5,h6,p") %>% 
  html_text()
nodes
```

Un peu de nettoyage... par exemple pour enlever toutes les refs de type [xx] dans le texte.

On recherche une **expression régulière**

[article blog expressions régulières](https://perso.ens-lyon.fr/lise.vaudor/strings-et-expressions-regulieres/)

```{r}
stringr::str_replace_all("bliblibloublou [23] et puis blablabla [58]",
                         "\\[\\d*\\]",#tous les nombres entre crochets
                         "") 

```

```{r}
get_text=function(link){ 
  html=read_html(link) 
  texts=html_nodes(html,"h1, h2, h3, h4, h5, h6, p") %>%
    purrr::map(html_text) %>%
    stringr::str_replace_all("\\[\\d*\\]","") 
  text=paste0(texts,collapse=" \n")
  return(text) 
}

get_text(link)
```

# Récupération du texte de tous les articles (sur Elisée Reclus)

On va appliquer la fonction get_text() définie ci-dessus sur l'ensemble des articles sur Elisée Reclus

[article blog sur itérations avec purrr](https://perso.ens-lyon.fr/lise.vaudor/iterer-des-fonctions-avec-purrr/)

```{r}
focus_ER=focus_ER %>% 
  mutate(text=purrr::map_chr(link,get_text))
#saveRDS(focus_ER,"data/focus_ER.RDS")
focus_ER %>% 
  head()
```

# Traduction

## Codes langue => codes langue Google

Récupérons le code-langue de chaque article

```{r}
focus_ER=focus_ER %>% 
  mutate(lang=stringr::str_extract(link,
                                      "(?<=https://)([[:alpha:]]|-)*(?=\\.)")) %>% 
  #le code langue= un assemblage de lettres et tiret, précédé de https: et suivi d'un point
  mutate(lang=case_when(lang=="simple"~"en",
                           TRUE~lang)) %>% 
     # convert language to Google Translate language code
  mutate(lang_google=case_when(lang=="zh"~"zh-CN",
                               lang=="zh-yue"~"zh-CN",
                               lang=="zh-classical"~"zh-CN",
                               lang=="zh-min-nan"~"zh-CN",
                               lang=="ar"~"hy",
                               lang=="bh"~"hi",
                               lang=="bjn"~"id",
                               TRUE~lang))
```

## Traduction par Google Translate / polyglotr

```{r}
library(polyglotr)

focus_ER$lang_google[1]
focus_ER$text[1]
google_translate_long_text(source_language=focus_ER$lang_google[1],  
                           target_language="fr",
                           focus_ER$text[1])
```

```{r}
num=which(focus_ER$lang=="en")
google_translate_long_text(source_language=focus_ER$lang_google[1],  
                           target_language="fr",
                           focus_ER$text[1])
```

Traduction de l'ensemble des articles:

```{r}
focus_ER_traduit=focus_ER %>% 
  mutate(texte_traduit=purrr::map2(.x=lang_google,
                                       .y=text,
                                       ~safely(google_translate_long_text)(source_language=.x,
                                                                           target_language="fr",
                                                                           text=.y))) 
#saveRDS(focus_ER_traduit,"data/focus_ER_traduit.RDS")
```


```{r}
focus_ER_traduit %>% 
  head()
```

```{r}
focus_ER_traduit %>% tail()

focus_ER_traduit=focus_ER_traduit %>% 
  mutate(texte_traduit=purrr::map(texte_traduit,
                                  "result")) %>% 
  mutate(texte_traduit=purrr::map_chr(texte_traduit,
                                      function(x){if(is.null(x)){return(NA)}else{return(x)}}))
```


## Remarque: gros nettoyage sur le corpus inondations


```{r}
clean_banners=function(text){
  banner=str_detect(text,"This article|this banner|your knowledge")
  if(banner & !is.na(text)){
  result=text %>% 
    str_replace("This article is.*\\n","") %>% 
    str_replace("This article is a draft concerning.*\\.\\n\\n","") %>% 
    str_replace("\\nedit - edit code - edit Wikidata\\n","") %>% 
    str_replace("You can share your knowledge by.*\\.","") %>% 
    str_replace("This article is based.*$","") %>% 
    str_replace("^(.|\\n)*If you think these points have been resolved, you can remove this banner and improve the formatting of another article\\.","")%>% 
    str_replace("This .*article.*\\. You can help Wikipedia by expanding it\\.","") %>% 
    str_replace("\\..*\\}","")
  }else{result=text}
  return(result)
}
```

```{r}
clean_text=function(text){
  patterns=c(
    "\\.[[^ ]]*",#ex .mw-output-parser
    "\\{[^\\{\\}]*\\}", #ex. {padding:0}
    "External links.{0,800}$",
    "Notes and references.{0,800}$",
    "(Links)* (Navigation)*.*In other languages$",
    "References External connections.{0,800}$",
    "See also.{0,800}$",
    "Literature External links.{0,800}$",
    "External references.{0,800}$",
    "Links.{0,200}$",
    "Notes ",
    "Related items",
    "External connections",
    "^Table of contents ",
    "Other projects ",
    "The text is available under the license \\\"Creative Commons name names-passing on under the same conditions\\\"",
    "[iI]nformation on the (authors|copyrights).*by clicking (on them)+",
    "The content may be subject to additional conditions",
    "Through use You agree to this website with the terms of use and the data protection directive",
    "By using this website, you agree to the terms of use and the data protection directive",
    "Additional literature Links",
    "Edit - Modify the code - Modify Wikidata ",
    "^Summary",
    "[Ff]ootnote",
    "Related topics$",
    "[Rr]eferences*.{0,50}$",
    "Sources*.{0,50}$",
    "See too.{0,50}$",
    "Related articles*.{0,50}$",
    "Notes*.{0,50}$",
    "\\{background-image: URL.*upload."
  )
  if(!(is.na(text)|is.nan(text)|is.null(text))){
    for (i in 1:length(patterns)){
      text=stringr::str_replace_all(text,patterns[i],"")
    }
  }else(text="")
  return(text)
}
```

# Tokénisation, lemmatisation (approche "sac de mots")

## Tokénisation

```{r}
library(tidytext)
focus_ER_tokens=focus_ER_traduit %>% 
  unnest_tokens(output="word",
                input=texte_traduit)
focus_ER_tokens %>% head()
```

## Lemmatisation

Voir site web de openlexicon ici:

[openlexicon](https://openlexicon.fr/datasets-info/Lexique382/README-Lexique.html)


Mon package `mixr` qui permet de charger le lexique (et faire d'autres traitements textuels que je montrerai ci-après):

[mixr](https://github.com/lvaudor/mixr)

```{r}
library(mixr)
lexicon_fr=get_lexicon("fr")
lexicon_fr
set.seed(33)
sample_n(lexicon_fr,10)
```

```{r}
focus_ER_tokens=focus_ER_tokens %>% 
  left_join(lexicon_fr,by=c("word"="word"))

head(focus_ER_tokens,100)
```

On filtre pour ne garder que des mots "signifiants" (exclusion des mots-outils):

```{r}
focus_ER_mots=focus_ER_tokens %>% 
  filter(type %in% c("ver","adj","nom")) %>% 
  filter(lemma!="reclus")
```

# Spécificités

Calcul des spécificités par le package `textometry` ("à la TXM"), appelé via `mixr`.

[Explication score de spécificité](https://perso.ens-lyon.fr/lise.vaudor/Descriptoire/_book/r%C3%A9sumer-linformation-par-des-m%C3%A9triques.html#spec)

On calcule les occurrences et spécificités des lemmes par langue

```{r}
spec=focus_ER_mots %>% 
  filter(lang %in% c("fr","zh","en","it","pt")) %>% 
  mixr::tidy_specificities(lemma,lang, top_spec=20)

head(spec)
```

```{r,fig.height=6}
plot_specificities(spec,lemma,lang)
```

